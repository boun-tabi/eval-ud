\section{Background}
\label{sec:background}


\subsection{Universal Dependencies (UD) and BOUN Treebank}
\label{sec:background:ud}
% Busra
The UD Turkish BOUN treebank comprises 9,761 UD-style annotated sentences from various domains, such as biographical texts, newspaper articles, essays, instructional texts, and more.
These sentences were randomly extracted from the Turkish National Corpus~\cite{aksan2012construction}, encompassing different registers.
Importantly, the BOUN treebank data faithfully captures the unique characteristics of Turkish, which include its relatively free word order and licensing pro-drop in addition to object drop.
For a comprehensive analysis of word order distribution within the BOUN treebank, refer to Table \ref{tab:wordord}.

\begin{table}[h]
    \centering
    \begin{tabular}{|c|S[table-format=4]|S|} \hline 
         \multicolumn{1}{|c|}{\textbf{Order}}&  \multicolumn{1}{c|}{\textbf{Count}}& \multicolumn{1}{c|}{\textbf{\%}}\\ \hline 
         OV&  5744& 37.21\\ \hline 
         SV&  5416& 35.09\\ \hline 
         SOV&  1456& 9.43\\ \hline 
         VS&  1116& 7.23\\ \hline 
         VO&  714& 4.63\\ \hline 
         OVS&  549& 3.56\\ \hline 
         VSO&  165& 1.07\\ \hline 
         SVO&  144& 0.93\\ \hline 
         OSV&  109& 0.71\\ \hline 
 VOS& 23&0.15\\ \hline
    \end{tabular}
    \caption{Word order distribution in the BOUN treebank. \textit{S: Subject, O: Object, V: Verb}}
    \label{tab:wordord}
\end{table}

The older version of the UD Turkish BOUN treebank, v2.8~\cite{turk2022resources}, had undergone a semi-automated annotation process in its inception.
Initially, sentences were parsed and syntactically annotated using~\citeauthor{kanerva2018turku}'s parsing pipeline, while morphological annotations were applied using~\citeauthor{sak2011resources}'s morphological disambiguator.
These annotations were then meticulously reviewed and improved by native Turkish speakers.
It's worth noting that there were no newly introduced annotation practices or deviations from the Universal Dependencies (UD) standards in terms of annotation or lemmatization in this version.

This particular version suffered from notable limitations in terms of abstraction and expressiveness, primarily due to the disparities between the annotation framework of UD and the morphologically rich and highly syncretic nature of the Turkish language.
Additionally, the prevalent use of morphophonologically null morphemes, such as the 0-morpheme copula, and highly productive derivational processes in Turkish contributed to these challenges.

UD, initially developed by speakers of morphologically sparse and highly isolating languages like Indo-European languages, adheres to a rigid and specific approach regarding word structure, derivation, lemma splitting, and null morphemes.
This approach considers derivational processes and affixes to be opaque, based on the observation that in most languages derivational affixes typically precede inflectional ones.
In contrast, languages such as Turkish exhibit a different pattern.
For example, \textit{-ki} which derives pronominals can follow genitive and locative case suffixes which are inflectional and further inflectional material such as the plural suffix can be applied to the stem derived by \textit{-ki} as shown in~\ref{evdekiler}:

\begin{exe}
\label{evdekiler}
    \ex 
    \gll ev -de -ki -ler \\
    home LOCATIVE ki PLURAL \\
    \glt \textit{`{[the ones]} at home}' \\
\end{exe}

Additionally, the UD framework lacks a consistent and unified approach to handling morphophonologically null morphemes.
As a result, languages featuring these morphemes have resorted to diverse strategies for annotation~\cite{marton2013dependency, ravishankar2017universal, dyer2022new}.
These limitations within the UD framework have been recognized, prompting discussions and debates within the UD community~\cite{gerdes2016dependency}.
Some topics like wordhood and word segmentation~\cite{seyoum2018universal} remain subjects of ongoing debate in this context.
In addition, researchers adopt different strategies to mitigate the shortcomings of UD in handling of the morphologically rich languages~\cite{more2016data, vincze2017universal, seyoum2018universal}, function words~\cite{osborne2019status}, and various other language-specific phenomena like case-drop~\cite{sundar-ram-lalitha-devi-2021-dependency-parsing}.

Version 2.11 of the BOUN treebank addresses the challenges related to null morphemes, lemmatization, intertwining derivational and inflectional processes, and syncretism within the Turkish language, all while staying as faithful as possible to the UD framework.
The primary objective behind creating v2.11 is to enhance the representational capabilities and both theoretical and practical accuracy of the BOUN treebank.

To achieve this goal, the entire treebank underwent a meticulous manual reannotation process, conducted by linguists who are native speakers of Turkish with domain expertise.
Throughout this reannotation process, issues such as lemmatization errors, inconsistencies in dependency annotations, and the presence of missing or extraneous morphological features were systematically addressed by the annotators.
Furthermore, novel annotation strategies were introduced (for more in-depth discussions of these strategies, please refer to~\citeauthor{marcsan2022enhancements} and~\citeauthor{bedir2021overcoming}).

\subsection{Large Language Models}
% Onur
The term ``Large Language Model'' refers to many aspects of a specific family of neural networks in a single phrase. 
The term ``large'' is relative, given that there is a larger one of every model. 
Moreover, we do not usually use earlier language models in the same way we use this type of language models.

Large language models are called ``large'' because they are large in the sense that they have tens of billions of parameters (e.g. 70B in Llama2~\cite{touvron2023llama2}, 180B in Falcon~\cite{falconllm}, 540B in PaLM~\cite{chowdhery2022palm}), compare this to the figure of 213M of the biggest neural network based model for NLP in 2017~\cite{vaswani2017attention}.
Large language models are called ``language models'' because they are set up so that they can be used to predict the next word in a given sequence of words in addition to providing a fixed-length vector that represents the given sequence.

The models used in this paper are Transformer-based decoder-only architectures that were trained with corpora that usually contain trillions of words. 
\textit{Transformer} is a model that proposes to represent each position in a given sequence as a function of the other words in the sequence~\cite{vaswani2017attention}. 
Moreover, this mechanism is implemented mostly through matrix multiplications which can be computed efficiently compared to previous approaches that usually involve sequential computation.

The literature in this area shows that these models are able to solve tasks that were not specifically included in the training phase, such as summarization, question answering, mathematical reasoning, even made up tasks like reversing the sequence of characters that make up a word~\cite{brown2020language}.

The training of LLMs involve a high level of computational resources as the architectures usually include billions of trainable parameters.  
Research funding to conduct a training operation of this magnitude is usually hard to obtain. 
This causes researchers to rely on organizations that serve these models with mostly commercial interests (\textit{OpenAI} and \textit{Hugging Face}~\cite{openai,huggingface}). This requirement restricts access to these models through the API services these organizations provide. 

\subsection{Evaluation of UD Resources}
Evaluating language resources is a critical stage in ensuring quality, fine-tuning, and optimization. That is why significant efforts have been dedicated to evaluating and assessing UD-style treebanks~\cite{nivre2017universal}. These endeavors typically revolve around two primary aspects:
\begin{itemize}
    \item Assessing the quality, consistency, and accuracy of annotations.
    \item Evaluating the performance of NLP systems trained on these resources in downstream tasks.
\end{itemize}

To evaluate annotation quality and accuracy, various metrics are employed, including head-attachment score (UAS, LAS), Kappa~\cite{mchugh2012interrater}, and various other inter-annotator agreement metrics.
These metrics are designed to assess the consistency among different annotators who contribute to the same resource or to gauge the overall consistency across language resources in the same language that are annotated by different teams~\cite{tyers2017assessment, grunewald2020unifying}. 

The other aspect frequently involves utilizing a natural language processing application for a downstream task, such as parsing, POS tagging, named entity recognition, or semantic role labeling.
With the emergence of more semantically-oriented approaches and foundation models, LLMs have been gaining prominence in UD evaluation tasks. 

One important point to note here is that the majority, if not all, of UD evaluation tasks involving LLMs assess the abilities of these models in tasks like parsing~\cite{al2023fine, kanerva2020dependency}, classification, learning and retaining syntax~\cite{kulmizev2020neural, limisiewicz2020universal}, performing cross-linguistic tasks~\cite{ahmad2019cross} or making generalizations about the structure of language~\cite{mccoy2019berts, yang2019exploring} using annotated UD data.
The use of LLMs to evaluate the expressive capabilities and annotation accuracy of UD resources, or any language resources for that matter, remains relatively unexplored.