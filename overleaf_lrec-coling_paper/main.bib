@misc{openai,
  author = {OpenAI},
  title = {{Models - OpenAI API}},
  howpublished = "\url{https://platform.openai.com/docs/models}",
  year = {2023}, 
  note = "[Online; accessed 19-October-2023]"
}

@misc{claude,
  author = {Anthropic},
  title = {{Product \ Anthropic}},
  howpublished = "\url{https://www.anthropic.com/product}",
  year = {2023}, 
  note = "[Online; accessed 19-October-2023]"
}

@misc{poe,
  author = {Poe},
  title = {{Welcome to Poe for Developers - Documentation}},
  howpublished = "\url{https://developer.poe.com}",
  year = {2023}, 
  note = "[Online; accessed 19-October-2023]"
}

% bert
@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

% gpt
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

% chatgpt
@misc{openai2021gpt35,
  author = {OpenAI},
  title = {GPT-3.5 (or ChatGPT) Language Model},
  year = {2021},
  howpublished = {\url{https://www.openai.com/chatgpt}},
  note = {Accessed on 19-October-2023}
}
% openai2021gpt35

% llama
@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

% llama2
@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

% python
@misc{python,
  author    = {Python Software Foundation},
  title     = {Python Language Reference},
  version   = {3.9},
  year      = {2022},
  url       = {https://www.python.org/},
  note      = {Accessed: 2023-10-19}
}


@article{Behre2022TRScoreAN,
  title={TRScore: A Novel GPT-based Readability Scorer for ASR Segmentation and Punctuation model evaluation and selection},
  author={Piyush Behre and S.S. Tan and Amy Shah and Harini Kesavamoorthy and Shuangyu Chang and Fei Zuo and Chris Basoglu and Sayan D. Pathak},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.15104},
  url={https://api.semanticscholar.org/CorpusID:253157741}
}

@article{Pelaez2023LargeScaleTA,
  title={Large-Scale Text Analysis Using Generative Language Models: A Case Study in Discovering Public Value Expressions in AI Patents},
  author={Sergio Pelaez and Gaurav Verma and Barbara Ribeiro and Philip Shapira},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.10383},
  url={https://api.semanticscholar.org/CorpusID:258741354}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

% cross-lingual part-of-speech tagging
@inproceedings{parvez-chang-2021-evaluating,
    title = "Evaluating the Values of Sources in Transfer Learning",
    author = "Parvez, Md Rizwan  and
      Chang, Kai-Wei",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.402",
    doi = "10.18653/v1/2021.naacl-main.402",
    pages = "5084--5116",
    abstract = "Transfer learning that adapts a model trained on data-rich sources to low-resource targets has been widely applied in natural language processing (NLP). However, when training a transfer model over multiple sources, not every source is equally useful for the target. To better transfer a model, it is essential to understand the values of the sources. In this paper, we develop , an efficient source valuation framework for quantifying the usefulness of the sources (e.g., ) in transfer learning based on the Shapley value method. Experiments and comprehensive analyses on both cross-domain and cross-lingual transfers demonstrate that our framework is not only effective in choosing useful transfer sources but also the source values match the intuitive source-target similarity.",
}

% semantic parsing
@inproceedings{reddy-etal-2017-universal,
    title = "Universal Semantic Parsing",
    author = {Reddy, Siva  and
      T{\"a}ckstr{\"o}m, Oscar  and
      Petrov, Slav  and
      Steedman, Mark  and
      Lapata, Mirella},
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1009",
    doi = "10.18653/v1/D17-1009",
    pages = "89--101",
    abstract = "Universal Dependencies (UD) offer a uniform cross-lingual syntactic representation, with the aim of advancing multilingual applications. Recent work shows that semantic parsing can be accomplished by transforming syntactic dependencies to logical forms. However, this work is limited to English, and cannot process dependency graphs, which allow handling complex phenomena such as control. In this work, we introduce UDepLambda, a semantic interface for UD, which maps natural language to logical forms in an almost language-independent fashion and can process dependency graphs. We perform experiments on question answering against Freebase and provide German and Spanish translations of the WebQuestions and GraphQuestions datasets to facilitate multilingual evaluation. Results show that UDepLambda outperforms strong baselines across languages and datasets. For English, it achieves a 4.9 F1 point improvement over the state-of-the-art on GraphQuestions.",
}

% language identification
@inproceedings{toftrup-etal-2021-reproduction,
    title = "A reproduction of Apple{'}s bi-directional {LSTM} models for language identification in short strings",
    author = "Toftrup, Mads  and
      Asger S{\o}rensen, S{\o}ren  and
      Ciosici, Manuel R.  and
      Assent, Ira",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-srw.6",
    doi = "10.18653/v1/2021.eacl-srw.6",
    pages = "36--42",
    abstract = "Language Identification is the task of identifying a document{'}s language. For applications like automatic spell checker selection, language identification must use very short strings such as text message fragments. In this work, we reproduce a language identification architecture that Apple briefly sketched in a blog post. We confirm the bi-LSTM model{'}s performance and find that it outperforms current open-source language identifiers. We further find that its language identification mistakes are due to confusion between related languages.",
}

@article{de2021universal,
  title={Universal dependencies},
  author={De Marneffe, Marie-Catherine and Manning, Christopher D and Nivre, Joakim and Zeman, Daniel},
  journal={Computational linguistics},
  volume={47},
  number={2},
  pages={255--308},
  year={2021},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{turk2022resources,
  title={Resources for {T}urkish dependency parsing: Introducing the {BOUN} treebank and the {BoAT} annotation tool},
  author={T{\"u}rk, Utku and Atmaca, Furkan and {\"O}zate{\c{s}}, {\c{S}}aziye Bet{\"u}l and Berk, G{\"o}zde and Bedir, Seyyit Talha and K{\"o}ksal, Abdullatif and Ba{\c{s}}aran, Balk{\i}z {\"O}zt{\"u}rk and G{\"u}ng{\"o}r, Tunga and {\"O}zg{\"u}r, Arzucan},
  journal={Language Resources and Evaluation},
  pages={1--49},
  year={2022},
  publisher={Springer}
}

@inproceedings{marcsan2022enhancements,
  title = {Enhancements to the {BOUN} Treebank Reflecting the Agglutinative Nature of {T}urkish},
  author = {Mar{\c{s}}an, B{\"u}{\c{s}}ra and Akkurt, Salih Furkan and {\c{S}}en, Muhammet and G{\"u}rb{\"u}z, Merve and G{\"u}ng{\"o}r, Onur and {\"O}zate{\c{s}}, {\c{S}}aziye Bet{\"u}l and {\"U}sk{\"u}darl{\i}, Suzan and {\"O}zg{\"u}r, Arzucan and G{\"u}ng{\"o}r, Tunga and {\"O}zt{\"u}rk, Balk{\i}z},
  booktitle = {Proceedings of the {ALTNLP} The International Conference and workshop on Agglutinative Language Technologies as a challenge of Natural Language Processing},
  group = {conference},
  link = {https://ceur-ws.org/Vol-3315/paper08.pdf},
  pages = {71-860},
  month = jun,
  day = {7-8},
  year = {2022}
}

@inproceedings{bedir2021overcoming,
  title={Overcoming the challenges in morphological annotation of {T}urkish in universal dependencies framework},
  author={Bedir, Talha and {\c{S}}ahin, Karahan and G{\"u}ng{\"o}r, Onur and Uskudarli, Suzan and {\"O}zg{\"u}r, Arzucan and G{\"u}ng{\"o}r, Tunga and Ba{\c{s}}aran, Balk{\i}z {\"O}zt{\"u}rk},
  booktitle={Proceedings of The Joint 15th Linguistic Annotation Workshop (LAW) and 3rd Designing Meaning Representations (DMR) Workshop},
  pages={112--122},
  year={2021}
}

@article{de2019dependency,
  title={Dependency grammar},
  author={De Marneffe, Marie-Catherine and Nivre, Joakim},
  journal={Annual Review of Linguistics},
  volume={5},
  pages={197--218},
  year={2019},
  publisher={Annual Reviews}
}

@article{debusmann2000introduction,
  title={An introduction to dependency grammar},
  author={Debusmann, Ralph},
  journal={Hausarbeit fur das Hauptseminar Dependenzgrammatik SoSe},
  volume={99},
  number={1},
  pages={16},
  year={2000}
}

@article{bauer1979some,
  title={Some thoughts on dependency grammar},
  author={Bauer, Laurie},
  year={1979},
  publisher={Walter de Gruyter, Berlin/New York Berlin, New York}
}

@inproceedings{aksan2012construction,
  title={Construction of the {T}urkish National Corpus ({TNC})},
  author={Aksan, Yesim and Aksan, Mustafa and Koltuksuz, Ahmet and Sezer, Taner and Mersinli, {\"U}mit and Demirhan, Umut Ufuk and Yilmazer, Hakan and Atasoy, G{\"u}ls{\"u}m and {\"O}z, Seda and Yildiz, Ipek and others},
  booktitle={LREC},
  pages={3223--3227},
  year={2012}
}

@article{sak2011resources,
  title={Resources for {T}urkish morphological processing},
  author={Sak, Ha{\c{s}}im and G{\"u}ng{\"o}r, Tunga and Sara{\c{c}}lar, Murat},
  journal={Language resources and evaluation},
  volume={45},
  pages={249--261},
  year={2011},
  publisher={Springer}
}

@inproceedings{kanerva2018turku,
  title={Turku neural parser pipeline: An end-to-end system for the {CoNLL} 2018 shared task},
  author={Kanerva, Jenna and Ginter, Filip and Miekka, Niko and Leino, Akseli and Salakoski, Tapio},
  booktitle={Proceedings of the CoNLL 2018 Shared Task: Multilingual parsing from raw text to universal dependencies},
  pages={133--142},
  year={2018}
}

@inproceedings{seyoum2018universal,
    title = "{U}niversal {D}ependencies for {A}mharic",
    author = "Seyoum, Binyam Ephrem  and
      Miyao, Yusuke  and
      Mekonnen, Baye Yimam",
    booktitle = "Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)",
    month = may,
    year = "2018",
    address = "Miyazaki, Japan",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L18-1350",
}

@inproceedings{vincze2017universal,
  title={Universal Dependencies and Morphology for Hungarian-and on the Price of Universality},
  author={Vincze, Veronika and Simk{\'o}, Katalin Ilona and Sz{\'a}nt{\'o}, Zsolt and Farkas, Rich{\'a}rd},
  year={2017},
  organization={Association for Computational Linguistics}
}

@inproceedings{more2016data,
  title={Data-driven morphological analysis and disambiguation for morphologically rich languages and universal dependencies},
  author={More, Amir and Tsarfaty, Reut},
  booktitle={Proceedings of COLING 2016, the 26th international conference on computational linguistics: Technical papers},
  pages={337--348},
  year={2016}
}

@article{osborne2019status,
  title={The status of function words in dependency grammar: A critique of Universal Dependencies (UD)},
  author={Osborne, Timothy and Gerdes, Kim},
  journal={Glossa: a journal of general linguistics (2016-2021)},
  year={2019}
}

@inproceedings{sundar-ram-lalitha-devi-2021-dependency-parsing,
    title = "Dependency Parsing in a Morphological rich language, {T}amil",
    author = "Sundar Ram, Vijay  and
      Lalitha Devi, Sobha",
    booktitle = "Proceedings of the First Workshop on Parsing and its Applications for Indian Languages",
    month = dec,
    year = "2021",
    address = "NIT Silchar, India",
    publisher = "NLP Association of India (NLPAI)",
    url = "https://aclanthology.org/2021.pail-1.3",
    pages = "20--26",
    abstract = "Dependency parsing is the process of analysing the grammatical structure of a sentence based on the dependencies between the words in a sentence. The annotation of dependency parsing is done using different formalisms at word-level namely Universal Dependencies and chunk-level namely AnnaCorra. Though dependency parsing is deeply dealt in languages such as English, Czech etc the same cannot be adopted for the morphologically rich and agglutinative languages. In this paper, we discuss the development of a dependency parser for Tamil, a South Dravidian language. The different characteristics of the language make this task a challenging task. Tamil, a morphologically rich and agglutinative language, has copula drop, accusative and genitive case drop and pro-drop. Coordinative constructions are introduced by affixation of morpheme {`}um{'}. Embedded clausal structures are common in relative participle and complementizer clauses. In this paper, we have discussed our approach to handle some of these challenges. We have used Malt parser, a supervised learning- approach based implementation. We have obtained an accuracy of 79.27{\%} for Unlabelled Attachment Score, 73.64{\%} for Labelled Attachment Score and 68.82{\%} for Labelled Accuracy.",
}

@inproceedings{ravishankar2017universal,
  title={A universal dependencies treebank for Marathi},
  author={Ravishankar, Vinit},
  booktitle={Proceedings of the 16th international workshop on treebanks and linguistic theories},
  pages={190--200},
  year={2017}
}

@inproceedings{dyer2022new,
  title={New syntactic insights for automated Wolof Universal Dependency parsing},
  author={Dyer, Bill},
  booktitle={Proceedings of the Fifth Workshop on the Use of Computational Methods in the Study of Endangered Languages},
  pages={5--12},
  year={2022}
}

@article{marton2013dependency,
  title={Dependency parsing of Modern Standard Arabic with lexical and inflectional features},
  author={Marton, Yuval and Habash, Nizar and Rambow, Owen},
  journal={Computational Linguistics},
  volume={39},
  number={1},
  pages={161--194},
  year={2013},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@inproceedings{gerdes2016dependency,
  title={Dependency annotation choices: Assessing theoretical and practical issues of universal dependencies},
  author={Gerdes, Kim and Kahane, Sylvain},
  booktitle={LAW X (2016) The 10th Linguistic Annotation Workshop: 131},
  year={2016}
}

@article{Liu2023GEvalNE,
  title={G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment},
  author={Yang Liu and Dan Iter and Yichong Xu and Shuo Wang and Ruochen Xu and Chenguang Zhu},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.16634},
  url={https://api.semanticscholar.org/CorpusID:257804696}
}

@article{wei2022chainCoT,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{Xu2023INSTRUCTSCORETE,
  title={INSTRUCTSCORE: Towards Explainable Text Generation Evaluation with Automatic Feedback},
  author={Wenda Xu and Danqing Wang and Liangming Pan and Zhenqiao Song and Markus Freitag and William Yang Wang and Lei Li},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.14282},
  url={https://api.semanticscholar.org/CorpusID:258841553}
}

@inproceedings{moore2023assessing,
  title={Assessing the quality of multiple-choice questions using gpt-4 and rule-based methods},
  author={Moore, Steven and Nguyen, Huy A and Chen, Tianying and Stamper, John},
  booktitle={European Conference on Technology Enhanced Learning},
  pages={229--245},
  year={2023},
  organization={Springer}
}

@misc{falconllm,
  author = {TII},
  title = {{Falcon LLM}},
  howpublished = "\url{https://falconllm.tii.ae/}",
  year = {2023}, 
  note = "[Online; accessed 21-October-2023]"
}

@misc{huggingface,
  author = {{Hugging Face}},
  title = {{Hugging Face}},
  howpublished = "\url{https://huggingface.co/}",
  year = {2023}, 
  note = "[Online; accessed 21-October-2023]"
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@inproceedings{grunewald2020unifying,
  title={Unifying the Treatment of Preposition-Determiner Contractions in German Universal Dependencies Treebanks},
  author={Gr{\"u}newald, Stefan and Friedrich, Annemarie},
  booktitle={Proceedings of the Fourth Workshop on Universal Dependencies (UDW 2020)},
  pages={94--98},
  year={2020}
}

@article{tyers2017assessment,
  title={An assessment of Universal Dependency annotation guidelines for Turkic languages},
  author={Tyers, Francis and Washington, Jonathan and {\c{C}}{\"o}ltekin, {\c{C}}a{\u{g}}r{\i} and Makazhanov, Aibek},
  year={2017},
  publisher={Tatarstan Academy of Sciences}
}

@article{mchugh2012interrater,
  title={Interrater reliability: the kappa statistic},
  author={McHugh, Mary L},
  journal={Biochemia medica},
  volume={22},
  number={3},
  pages={276--282},
  year={2012},
  publisher={Medicinska naklada}
}

@inproceedings{nivre2017universal,
  title={Universal dependency evaluation},
  author={Nivre, Joakim and Fang, Chiao-Ting},
  booktitle={Proceedings of the NoDaLiDa 2017 Workshop on Universal Dependencies (UDW 2017)},
  pages={86--95},
  year={2017}
}

@article{mccoy2019berts,
  title={BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance},
  author={McCoy, R Thomas and Min, Junghyun and Linzen, Tal},
  journal={arXiv preprint arXiv:1911.02969},
  year={2019}
}

@inproceedings{yang2019exploring,
  title={Exploring pre-trained language models for event extraction and generation},
  author={Yang, Sen and Feng, Dawei and Qiao, Linbo and Kan, Zhigang and Li, Dongsheng},
  booktitle={Proceedings of the 57th annual meeting of the association for computational linguistics},
  pages={5284--5294},
  year={2019}
}

@article{ahmad2019cross,
  title={Cross-lingual dependency parsing with unlabeled auxiliary languages},
  author={Ahmad, Wasi Uddin and Zhang, Zhisong and Ma, Xuezhe and Chang, Kai-Wei and Peng, Nanyun},
  journal={arXiv preprint arXiv:1909.09265},
  year={2019}
}

@article{al2023fine,
  title={Fine-Tuning BERT-Based Pre-Trained Models for Arabic Dependency Parsing},
  author={Al-Ghamdi, Sharefah and Al-Khalifa, Hend and Al-Salman, Abdulmalik},
  journal={Applied Sciences},
  volume={13},
  number={7},
  pages={4225},
  year={2023},
  publisher={MDPI}
}

@article{kanerva2020dependency,
  title={Dependency parsing of biomedical text with BERT},
  author={Kanerva, Jenna and Ginter, Filip and Pyysalo, Sampo},
  journal={BMC bioinformatics},
  volume={21},
  pages={1--12},
  year={2020},
  publisher={Springer}
}

@article{kulmizev2020neural,
  title={Do neural language models show preferences for syntactic formalisms?},
  author={Kulmizev, Artur and Ravishankar, Vinit and Abdou, Mostafa and Nivre, Joakim},
  journal={arXiv preprint arXiv:2004.14096},
  year={2020}
}

@article{limisiewicz2020universal,
  title={Universal dependencies according to BERT: both more specific and more general},
  author={Limisiewicz, Tomasz and Rosa, Rudolf and Mare{\v{c}}ek, David},
  journal={arXiv preprint arXiv:2004.14620},
  year={2020}
}

@article{liang2022holistichelm,
  title={Holistic evaluation of language models},
  author={Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others},
  journal={arXiv preprint arXiv:2211.09110},
  year={2022}
}