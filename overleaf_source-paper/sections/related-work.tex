\section{Related Work}
\label{sec:related-work}

% Onur

Evaluating large language models is not as straight-forward as evaluating other ML models.
Evaluation of traditional NLP tasks might seem as simple as asking the LLM to give the name of the correct class among the possible classes in a text classification task. However, even this is complicated because the exact prompt that is used may affect the outcome.
On the other hand, as LLMs are general-purpose AI tools that are expected to behave in many ways, a complete evaluation should include more than a number of accuracy results among several traditional NLP tasks.
To alleviate this issue, it is suggested to assess the LLMs with scenarios that cover many dimensions like the domain, the intended users, the timeframe of the test data, and the language in addition to the task itself.
In~\cite{liang2022holistichelm}, it is also noted that the metrics like calibration, robustness, fairness, bias, toxicity, and efficiency should also be included.

GPT3~\cite{brown2020language} was utilized in several different ways for a number of evaluation tasks.
One of them involves using the log-probabilities reported by the system to arrive at a readability measure that correlates with the  readability scores given by humans~\cite{Behre2022TRScoreAN}.
A framework that uses large language models with Chain-of-Thought paradigm~\cite{wei2022chainCoT} to evaluate natural language generation outputs achieves a high Spearman correlation with human judgments for the summarization task~\cite{Liu2023GEvalNE}.
An explainable evaluation metric~\cite{Xu2023INSTRUCTSCORETE} is developed using LLaMA~\cite{touvron2023llama} and GPT4~\cite{openai2021gpt35} without using human feedback that shows better performance than other text generation metrics.
GPT-4 was also used in evaluating automatically generated questions~\cite{moore2023assessing}.
